{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27649356-2bb7-4d77-9385-12b83847b921",
   "metadata": {},
   "source": [
    "# Predicción de Litologías con Aprendizaje Automático\n",
    "\n",
    "Este proyecto tiene como objetivo predecir litologías a partir de datos geológicos usando técnicas de imputación, transformación de señales y modelos de clasificación. A continuación se detallan los pasos realizados y se presenta una demostración funcional del código.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9763c2-4dcb-4659-ba81-8cd2a09cc8e8",
   "metadata": {},
   "source": [
    "Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "495db2d5-6adb-4ee6-ad2a-3212614c01e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IterativeImputer\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExtraTreesRegressor\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpywt\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pywt\n",
    "import pickle\n",
    "import precond\n",
    "import wavelet_transform\n",
    "import feature_augmentation\n",
    "import validation\n",
    "import imputation\n",
    "\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b77f231-ab10-441d-90ea-023aec733b77",
   "metadata": {},
   "source": [
    "Carga del Dataset y Estadísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eefc91-86ed-4daa-8cb4-25675c86731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "train = pd.read_csv('train.csv', sep=';')\n",
    "train_bkp = train.copy()\n",
    "train.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2e2fa-00ef-4830-a263-2d262ad9d783",
   "metadata": {},
   "source": [
    "## Preprocesamiento e Imputación\n",
    "\n",
    "Se realiza el preprocesamiento y la imputación de datos faltantes. Se codifican las variables categóricas y se seleccionan las características relevantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038fd2be-3c64-4c38-ad65-8b6e411d2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento\n",
    "# train = precond.precond_train(train)\n",
    "\n",
    "# Codificación\n",
    "# le = LabelEncoder()\n",
    "# le.fit(train['GROUP'])\n",
    "# train['GROUP'] = le.transform(train['GROUP'])\n",
    "# pickle.dump(le, open('labelencoder.pkl', 'wb'))\n",
    "\n",
    "# Reducción de columnas\n",
    "# train_imp = train.drop(['WELL','FORCE_2020_LITHOFACIES_LITHOLOGY','FORCE_2020_LITHOFACIES_CONFIDENCE'], axis=1)\n",
    "# train_imp, model_imp_list = imputation.imputer_train(train_imp, \"train_imp.csv\")\n",
    "# train_imp['WELL'] = train['WELL']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f951ebd-bd98-4438-8587-dffbd0efaad8",
   "metadata": {},
   "source": [
    "## Selección de Características, Transformaciones y Aumento\n",
    "\n",
    "Se seleccionan variables clave, se aplican transformaciones wavelet y se realiza el aumento de características.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f1209d-4a39-415e-952d-71ff47955b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['WELL','X_LOC','Y_LOC','Z_LOC','RDEP','GROUP','CALI','GR','RHOB','NPHI','PEF','DTC','SP','DRHO']\n",
    "# train_imp = train_imp[features]\n",
    "# train_imp['FORCE_2020_LITHOFACIES_LITHOLOGY'] = train['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "# train_imp = wavelet_transform.wavelet_transform(train_imp)\n",
    "\n",
    "# Eliminación del efecto hombro\n",
    "# train_imp.loc[train_imp['FORCE_2020_LITHOFACIES_LITHOLOGY'].shift(-1) != train_imp['FORCE_2020_LITHOFACIES_LITHOLOGY'].shift(1), 'FORCE_2020_LITHOFACIES_LITHOLOGY'] = np.nan\n",
    "# train_imp.dropna(subset=['FORCE_2020_LITHOFACIES_LITHOLOGY'], inplace=True)\n",
    "\n",
    "# Aumento\n",
    "# train_imp_aug = train_imp.drop(['WELL','FORCE_2020_LITHOFACIES_LITHOLOGY'], axis=1)\n",
    "# train_imp_aug = feature_augmentation.feat_aug(train_imp_aug, train_imp['WELL'], train_imp['Z_LOC'])\n",
    "# train_imp_aug = feature_augmentation.poly_feat(train_imp_aug)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c85253-5cd9-450b-af68-8cd4f3431f62",
   "metadata": {},
   "source": [
    "## Entrenamiento del Modelo\n",
    "\n",
    "Se entrena un modelo de clasificación XGBoost utilizando una división entrenamiento/prueba del conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d88c3-d15d-464e-8d8a-ee4410a448d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = train_imp_aug\n",
    "# y = train_imp['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=0)\n",
    "\n",
    "# model = XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=300, random_state=0)\n",
    "# model.fit(X_train, y_train)\n",
    "# print(\"Model trained\")\n",
    "\n",
    "# accuracy_split_test, pen_rel_split_test = validation.validation(X_test, y_test, train_imp, model, 'penalty_matrix.npy') \n",
    "# pickle.dump(model, open('model.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c138e2-5aa4-4bb2-9c9d-4cf2af05497c",
   "metadata": {},
   "source": [
    "## Predicción en Conjunto de Prueba Cerrado\n",
    "\n",
    "Se carga el conjunto cerrado, se preprocesa y se predicen las litologías utilizando el modelo entrenado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdecdfe-0f79-4ff3-a5a1-e36f7f542935",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv', sep=';')\n",
    "test_bkp = test.copy()\n",
    "\n",
    "test = precond.precond_test(test)\n",
    "\n",
    "le = pickle.load(open('labelencoder.pkl', 'rb'))\n",
    "test['GROUP'] = le.transform(test['GROUP'])\n",
    "\n",
    "test_imp = test.drop(['WELL'], axis=1)\n",
    "\n",
    "train_imp_csv = pd.read_csv('train_imp.csv', sep=';')\n",
    "test_imp = imputation.imputer_test(test_imp, train_imp_csv)\n",
    "\n",
    "test_imp['WELL'] = test['WELL']\n",
    "test_imp = test_imp[features]\n",
    "test_imp = wavelet_transform.wavelet_transform(test_imp)\n",
    "\n",
    "test_imp_aug = test_imp.drop(['WELL'], axis=1)\n",
    "test_imp_aug = feature_augmentation.feat_aug(test_imp_aug, test_imp['WELL'], test_imp['Z_LOC'])\n",
    "test_imp_aug = feature_augmentation.poly_feat(test_imp_aug)\n",
    "\n",
    "model = pickle.load(open('model.pkl', 'rb'))\n",
    "prediction = model.predict(test_imp_aug)\n",
    "\n",
    "test['PREDICTION'] = prediction\n",
    "test.loc[test['PREDICTION'].shift(-2) == test['PREDICTION'].shift(1), 'PREDICTION'] = test['PREDICTION'].shift(-2)\n",
    "test.loc[test['PREDICTION'].shift(-1) == test['PREDICTION'].shift(1), 'PREDICTION'] = test['PREDICTION'].shift(-1)\n",
    "test['PREDICTION'].iloc[-1] = prediction[-1]\n",
    "\n",
    "np.savetxt('GIR_TEAM_final_submission.csv', test['PREDICTION'].values, header='lithology', comments='', fmt='%i')\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
